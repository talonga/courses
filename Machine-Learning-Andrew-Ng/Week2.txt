Multivariate Linear Regression
h(x) = T0 + T1x1 + T2x2 ... Tnxn
h = T (Transpose) X
where T is a vector of all theta (T)
and X is a vector of all x

Feature Scaling
make sure features are on a similar scale, as will cause gradient descent to converge quicker
Mean normalization is to make features have approximately zero mean

Learning Rate Alpha
If gradient descent is working properly, then J(T) should decrease after every iteration.
If increase, likely alpha is too big.

Features and Polynomial Regression

Normal Equation
Solve for T immediately rather than iteratively
advantages and disadvantages over gradient descent

E.g. solve d/dx T = 0, but what if T is a vector?

for vector T,
T = X(transpose)X-1 * X(transpose)Y
or Octave : pinv(X' * X) * X' * Y

Gradient Descent : need to choose alpha, need many iteration. But works well even when n is large
Normal Equation : no need to choose alpha, don't need to iterate. Slow if n is very large.

What is X'X is non-invertible?
- redundant features (linearly dependent)
- too many features (e.g. no. of examples <= no. of features)
