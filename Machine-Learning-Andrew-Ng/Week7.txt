Support Vector Machines (SVM)

SVM will output prediction instead of probability
SVM = large margin classifiers?
A (cost function) + lambda B (regularization)
instead, use C * A + B, where C ~ 1/lambda (different representation)

Kernels
Gaussian kernel - similarity function
measure distance of x to landmark (l),
if x ~ l, then f1 = similarity(x, l) ~ 1
if x is far from l, then f1 ~ 0
each landmark is a new feature

use t0 + t1f1 + t2f2 + t3f3 as hypothesis

Based on similarity of a new "x" training example, perform prediction based on similarity to landmarks

for each x, compute new feature vector 
intuitively, checking each training example for 'closeness' or similiarity to every other training example

effectively, number of features = number of training examples

C ~ 1/lambda
large C
- small lambda - not much regularization (regularization exists to help with overfitting!)
- low bias, high variance - overfitting
- decrease C to help with overfitting

small C
- large lambda - a lot of regularization
- high bias, low variance - underfitting
- increase C to help with underfitting

large sigma^2
- features f vary more smoothly
- high bias, low variance - underfitting
- decrease sigma to help with underfitting

small signa,^2
- features f vary less smoothly
- low bias, high variance - overfitting
- increase sigma to help with overfitting

if n is large (relative to m) e.g. n = 10,000, m = 10 ... 1,000
use logistic regression, or SVM without a kernel

if n is small, m is intermediate e.g. n = 1 - 1,000, m = 10 ... 10,000
use SVM with gaussian kernel

if n is small, m is large e.g. n = 1 - 1,000, m = 50,000+
create / add more features, then use logistic regression or SVN without a kernel

neural network likely to work well for most of these settings, but may be slower to train